"""
èŠ‚ç‚¹å®šä¹‰
"""

from typing import Dict, Any, Literal
from datetime import datetime
from langchain_core.messages import SystemMessage, AIMessage, ToolMessage, HumanMessage

from ..config import (
    llm, decision_making_prompt, planning_prompt, agent_prompt, judge_prompt,
    survey_outline_prompt, survey_title_abstract_prompt, MAX_SURVEY_REFERENCE, SAVE_DIR
)
from ..models import AgentState, DecisionMakingOutput, JudgeOutput, TypeEnum, SurveySections
from ..tools import tools, tools_dict
from ..utils import format_tools_description
from ..services.paper_service import PaperService
from ..services.survey_service import SurveyService

decision_making_llm = llm.with_structured_output(DecisionMakingOutput)
agent_llm = llm.bind_tools(tools)
judge_llm = llm.with_structured_output(JudgeOutput)

# åˆå§‹åŒ–æœåŠ¡
paper_service = PaperService()
survey_service = SurveyService()


def decision_making_node(state: AgentState) -> Dict[str, Any]:
    """å†³ç­–èŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢å†³å®šæ˜¯å¦éœ€è¦ç ”ç©¶"""
    system_prompt = SystemMessage(content=decision_making_prompt)
    response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state["messages"])
    
    output = {"requires_research": response.requires_research, "type": response.type}
    if response.answer:
        output["messages"] = [AIMessage(content=response.answer)]
    return output


def router(state: AgentState) -> Literal["planning", "end"]:
    """è·¯ç”±å™¨ï¼šå°†ç”¨æˆ·æŸ¥è¯¢å¼•å¯¼åˆ°é€‚å½“çš„å·¥ä½œæµåˆ†æ”¯"""
    if state["requires_research"]:
        if state["type"] == TypeEnum.report:
            state["messages"][-1].content = f"æŸ¥è¯¢ {MAX_SURVEY_REFERENCE} ç¯‡èƒ½å¤Ÿè¾…åŠ©è®ºæ–‡ä¸»é¢˜ {state['messages'][-1].content} çš„ç›¸å…³å¯å¼•ç”¨è®ºæ–‡ã€‚\næ— è®ºè®ºæ–‡ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Œä½ çš„ä»»åŠ¡ä»…æœ‰æŸ¥è¯¢ã€‚"
        return "planning"
    else:
        return "end"


def planning_node(state: AgentState) -> Dict[str, Any]:
    """è§„åˆ’èŠ‚ç‚¹ï¼šåˆ›å»ºé€æ­¥è®¡åˆ’æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢"""
    system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))
    response = llm.invoke([system_prompt] + state["messages"])
    return {"messages": [response]}


def tools_node(state: AgentState) -> Dict[str, Any]:
    """å·¥å…·èŠ‚ç‚¹ï¼šåŸºäºè®¡åˆ’æ‰§è¡Œå·¥å…·"""
    outputs = []
    for tool_call in state["messages"][-1].tool_calls:
        tool_result = tools_dict[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=str(tool_result),
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}


def agent_node(state: AgentState) -> Dict[str, Any]:
    """æ™ºèƒ½åŠ©æ‰‹èŠ‚ç‚¹ï¼šä½¿ç”¨å¸¦å·¥å…·çš„LLMå›ç­”ç”¨æˆ·æŸ¥è¯¢"""
    system_prompt = SystemMessage(content=agent_prompt)
    response = agent_llm.invoke([system_prompt] + state["messages"])
    return {"messages": [response]}


def should_continue(state: AgentState) -> Literal["continue", "end"]:
    """æ£€æŸ¥æ™ºèƒ½åŠ©æ‰‹æ˜¯å¦åº”è¯¥ç»§ç»­æˆ–ç»“æŸ"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸæ‰§è¡Œ
    if last_message.tool_calls:
        return "continue"
    else:
        return "end"


def generate_survey_agent(state: AgentState) -> Dict[str, Any]:
    """ç”Ÿæˆè®ºæ–‡ç»¼è¿°èŠ‚ç‚¹ï¼šåŸºäºåè°ƒè€…-å·¥ä½œè€…æ¨¡å¼ç”Ÿæˆé«˜è´¨é‡ç»¼è¿°æŠ¥å‘Šï¼ˆåŸºæœ¬å®ç°æ€è·¯å¯ä»¥å‚è€ƒ `./llm-projects/Beginner/02_building_effective_agents/4_orchestrator-worker/graph_api.py`ï¼‰"""
    try:
        # 1. æå–åŸå§‹ç”¨æˆ·æŸ¥è¯¢ï¼ˆç»¼è¿°ä¸»é¢˜ï¼‰
        original_query = state["messages"][0].content
        print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆç»¼è¿°æŠ¥å‘Šï¼Œä¸»é¢˜ï¼š{original_query}")
        
        # 2. ä»ä¿å­˜ç›®å½•ä¸­æå–å·²ä¸‹è½½çš„è®ºæ–‡ä¿¡æ¯
        papers_info = paper_service.extract_paper_content_from_save_dir()
        print(f"ğŸ“š ä» {SAVE_DIR} ç›®å½•å‘ç° {len(papers_info)} ç¯‡å·²ä¸‹è½½çš„è®ºæ–‡")
        
        if not papers_info:
            return {
                "messages": [AIMessage(content="âš ï¸ æœªæ‰¾åˆ°å·²ä¸‹è½½çš„è®ºæ–‡ï¼Œæ— æ³•ç”Ÿæˆç»¼è¿°æŠ¥å‘Šã€‚è¯·å…ˆæœç´¢å’Œä¸‹è½½ç›¸å…³è®ºæ–‡ã€‚")]
            }
        
        # 3. åè°ƒè€…é˜¶æ®µï¼šåˆ†æè®ºæ–‡å¹¶è§„åˆ’ç»¼è¿°ç»“æ„
        print("ğŸ§  åè°ƒè€…é˜¶æ®µï¼šåˆ†æè®ºæ–‡å†…å®¹å¹¶è§„åˆ’ç»¼è¿°ç»“æ„...")
        paper_summaries = []
        for i, paper in enumerate(papers_info):
            print(f"  ğŸ“„ åˆ†æç¬¬ {i+1} ç¯‡è®ºæ–‡...")
            summary = paper_service.summarize_paper_content(paper['content'], original_query)
            paper_summaries.append(summary)
        
        # 4. ç”Ÿæˆç»¼è¿°å¤§çº²
        planner_llm = llm.with_structured_output(SurveySections)
        outline_prompt_text = survey_outline_prompt.format(
            topic=original_query,
            paper_summaries=chr(10).join([f"è®ºæ–‡{i+1}: {summary.title} - {summary.abstract[:200]}..." for i, summary in enumerate(paper_summaries)])
        )
        
        survey_outline = planner_llm.invoke([HumanMessage(content=outline_prompt_text)])
        print(f"ğŸ“‹ ç”Ÿæˆäº† {len(survey_outline.sections)} ä¸ªç»¼è¿°ç« èŠ‚:")
        for i, section in enumerate(survey_outline.sections):
            print(f"  {i+1}. {section.title}")

        print("âœï¸ å·¥ä½œè€…é˜¶æ®µï¼šå¹¶è¡Œç”Ÿæˆå„ç« èŠ‚è¯¦ç»†å†…å®¹...")
        
        # å¹¶è¡Œç”Ÿæˆç« èŠ‚å†…å®¹
        completed_sections = survey_service.parallel_generate_sections(
            survey_outline.sections, 
            original_query, 
            paper_summaries
        )
        
        # 6. åˆæˆå™¨é˜¶æ®µï¼šæ•´åˆæ‰€æœ‰ç« èŠ‚ç”Ÿæˆå®Œæ•´ç»¼è¿°
        print("ğŸ”— åˆæˆå™¨é˜¶æ®µï¼šæ•´åˆå®Œæ•´ç»¼è¿°æŠ¥å‘Š...")
        
        title_abstract_prompt_text = survey_title_abstract_prompt.format(
            topic=original_query,
            paper_count=len(paper_summaries)
        )
        
        title_abstract = llm.invoke([HumanMessage(content=title_abstract_prompt_text)])
        
        # æ•´åˆå®Œæ•´ç»¼è¿°
        full_survey = f"""
{title_abstract.content}

---

{chr(10).join(completed_sections)}

---

## å‚è€ƒæ–‡çŒ®

{chr(10).join([f"{i+1}. {summary.title}" for i, summary in enumerate(paper_summaries)])}

---

*æœ¬ç»¼è¿°åŸºäº {len(paper_summaries)} ç¯‡ç›¸å…³è®ºæ–‡ç”Ÿæˆï¼Œç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
        """
        
        # 7. ä¿å­˜ç»¼è¿°æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        survey_filename = f"survey_report_{timestamp}.md"
        
        try:
            with open(survey_filename, 'w', encoding='utf-8') as f:
                f.write(full_survey)
            print(f"ğŸ’¾ ç»¼è¿°æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{survey_filename}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»¼è¿°æŠ¥å‘Šæ—¶å‡ºé”™ï¼š{e}")
        
        # 8. è¿”å›ç»“æœ
        success_message = f"""
âœ… **ç»¼è¿°æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼**

ğŸ“Š **ç»Ÿè®¡ä¿¡æ¯ï¼š**
- åˆ†æè®ºæ–‡æ•°é‡ï¼š{len(paper_summaries)} ç¯‡
- ç»¼è¿°ç« èŠ‚æ•°é‡ï¼š{len(survey_outline.sections)} ä¸ª
- æŠ¥å‘Šä¿å­˜ä½ç½®ï¼š{survey_filename}

ğŸ“ **ç»¼è¿°é¢„è§ˆï¼š**
{full_survey[:10000]}...

ğŸ’¡ å®Œæ•´ç»¼è¿°æŠ¥å‘Šå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥è¿›ä¸€æ­¥ç¼–è¾‘å’Œå®Œå–„ã€‚
        """
        
        return {
            "messages": [AIMessage(content=success_message)]
        }
        
    except Exception as e:
        error_message = f"âŒ ç»¼è¿°ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}\n\nè¯·æ£€æŸ¥è®ºæ–‡ä¸‹è½½çŠ¶æ€å’Œç½‘ç»œè¿æ¥ã€‚"
        return {
            "messages": [AIMessage(content=error_message)]
        }


def judge_node(state: AgentState) -> Dict[str, Any]:
    """åˆ¤æ–­èŠ‚ç‚¹ï¼šè®©LLMè¯„ä¼°è‡ªå·±æœ€ç»ˆç­”æ¡ˆçš„è´¨é‡"""
    num_feedback_requests = state.get("num_feedback_requests", 0)
    
    # å¦‚æœLLMä¸‰æ¬¡æœªèƒ½æä¾›è‰¯å¥½ç­”æ¡ˆï¼Œåˆ™ç»“æŸæ‰§è¡Œ
    if num_feedback_requests >= 3:
        return {"is_good_answer": True}
    
    system_prompt = SystemMessage(content=judge_prompt)
    response: JudgeOutput = judge_llm.invoke([system_prompt] + state["messages"])
    
    output = {
        "is_good_answer": response.is_good_answer,
        "num_feedback_requests": num_feedback_requests + 1
    }
    if response.feedback:
        output["messages"] = [AIMessage(content=response.feedback)]
    return output


def final_answer_router(state: AgentState) -> Literal["planning", "end"]:
    """æœ€ç»ˆç­”æ¡ˆè·¯ç”±å™¨ï¼šç»“æŸå·¥ä½œæµæˆ–æ”¹è¿›ç­”æ¡ˆ"""
    if state["is_good_answer"]:
        if state["type"] == TypeEnum.report:
            state["messages"][0].content = f"{state['messages'][0].content}".replace(f"æŸ¥è¯¢ {MAX_SURVEY_REFERENCE} ç¯‡èƒ½å¤Ÿè¾…åŠ©è®ºæ–‡ä¸»é¢˜", "").replace("çš„ç›¸å…³å¯å¼•ç”¨è®ºæ–‡ã€‚\næ— è®ºè®ºæ–‡ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Œä½ çš„ä»»åŠ¡ä»…æœ‰æŸ¥è¯¢ã€‚", "")
            return "generate_survey"
        return "end"
    else:
        return "planning"
