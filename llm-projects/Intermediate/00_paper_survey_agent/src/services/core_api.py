"""
CORE APIå°è£…
"""

import os
import time
from typing import ClassVar, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from pydantic import BaseModel

from ..config import CORE_API_KEY


class CoreAPIWrapper(BaseModel):
    """CORE APIçš„ç®€å•å°è£…"""
    base_url: ClassVar[str] = "https://api.core.ac.uk/v3"
    api_key: ClassVar[str] = CORE_API_KEY
    top_k_results: int = 1

    def _get_search_response(self, query: str) -> Dict[str, Any]:
        """è·å–æœç´¢å“åº”"""
        url = f"{self.base_url}/search/works"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "q": query,
            "limit": self.top_k_results,
            "offset": 0,
            "scroll": False
        }
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(url, json=data, headers=headers, timeout=30)
                response.raise_for_status()
                original_output = response.json()
                
                # å¹¶è¡Œä¸‹è½½è®ºæ–‡
                download_urls = [result.get("downloadUrl") for result in original_output.get("results", []) if result.get("downloadUrl")]
                if download_urls:
                    print(f"ğŸš€ å¼€å§‹å¹¶è¡Œä¸‹è½½ {len(download_urls)} ç¯‡è®ºæ–‡...")
                    self._parallel_download_papers(download_urls)
                
                return original_output
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    raise Exception(f"CORE APIè¯·æ±‚å¤±è´¥: {e}")
    
    def _parallel_download_papers(self, download_urls: list[str], max_workers: int = 3) -> None:
        """
        å¹¶è¡Œä¸‹è½½å¤šç¯‡è®ºæ–‡
        
        Args:
            download_urls: ä¸‹è½½é“¾æ¥åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º3
        """
        def download_single_paper(url: str) -> tuple[str, bool, str]:
            """ä¸‹è½½å•ç¯‡è®ºæ–‡çš„åŒ…è£…å‡½æ•°"""
            try:
                from ..tools.download_tools import download_paper
                result = download_paper(url)
                return url, True, result
            except Exception as e:
                return url, False, str(e)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(download_single_paper, url): url for url in download_urls}
            
            completed_count = 0
            failed_count = 0
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    original_url, success, result = future.result()
                    if success:
                        completed_count += 1
                        print(f"  âœ… ä¸‹è½½å®Œæˆ ({completed_count}/{len(download_urls)}): {os.path.basename(original_url)}")
                    else:
                        failed_count += 1
                        print(f"  âŒ ä¸‹è½½å¤±è´¥ ({failed_count}/{len(download_urls)}): {result}")
                except Exception as e:
                    failed_count += 1
                    print(f"  âŒ ä¸‹è½½å¼‚å¸¸ ({failed_count}/{len(download_urls)}): {e}")
            
            print(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡: æˆåŠŸ {completed_count} ç¯‡, å¤±è´¥ {failed_count} ç¯‡")

    def search(self, query: str) -> str:
        """æœç´¢è®ºæ–‡"""
        response = self._get_search_response(query)
        results = response.get("results", [])
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"

        # æ ¼å¼åŒ–ç»“æœ
        docs = []
        for result in results:
            published_date_str = result.get('publishedDate') or result.get('yearPublished', '')
            authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])
            docs.append((
                f"* ID: {result.get('id', '')}\n"
                f"* æ ‡é¢˜: {result.get('title', '')}\n"
                f"* å‘è¡¨æ—¥æœŸ: {published_date_str}\n"
                f"* ä½œè€…: {authors_str}\n"
                f"* æ‘˜è¦: {result.get('abstract', '')}\n"
                f"* è®ºæ–‡ä¸‹è½½é“¾æ¥: {result.get('downloadUrl') or result.get('sourceFulltextUrls', '')}"
            ))
        return "\n-----\n".join(docs)
