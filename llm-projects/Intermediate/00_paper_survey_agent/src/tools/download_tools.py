"""
ä¸‹è½½å·¥å…·æ¨¡å—
"""

import os
import io
import time
import urllib3
import re
import uuid
from datetime import datetime
from urllib.parse import urlparse
import pdfplumber
from langchain_core.tools import tool

from ..config import SAVE_DIR


@tool("download-paper")
def download_paper(url: str) -> str:
    """ä»ç»™å®šURLä¸‹è½½ç‰¹å®šç§‘å­¦è®ºæ–‡ï¼ˆè‹¥åªç»™å®šè®ºæ–‡æ ‡é¢˜ï¼Œè¯·å…ˆä½¿ç”¨ `search_papers` å·¥å…·è¿›è¡ŒæŸ¥è¯¢è®ºæ–‡ï¼Œè·å–ä¸‹è½½URLï¼‰
    
    ç¤ºä¾‹ï¼š
    {"url": "https://sample.pdf"}
    
    è¿”å›ï¼š
        è®ºæ–‡å†…å®¹
    """
    try:
        http = urllib3.PoolManager(cert_reqs='CERT_NONE')
        
        # æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´é¿å…403é”™è¯¯
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }

        if "arxiv.org/abs" in url:
            url = url.replace("arxiv.org/abs", "arxiv.org/pdf")
        
        max_retries = 5
        for attempt in range(max_retries):
            response = http.request('GET', url, headers=headers)
            if 200 <= response.status < 300:
                pdf_file = io.BytesIO(response.data)

                save_dir = SAVE_DIR
                os.makedirs(save_dir, exist_ok=True)

                parsed_url = urlparse(url)
                filename = os.path.basename(parsed_url.path)
                # print(f"[DEBUG] PDFæ–‡ä»¶é¢„æœŸä¿å­˜æ–‡ä»¶å: {filename}")
                if not str(filename).endswith('.pdf'):
                    # å¦‚æœæ— æ³•ä»URLè·å–æ–‡ä»¶åï¼Œä½¿ç”¨æ—¶é—´æˆ³+uuid
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    unique_id = str(uuid.uuid4())
                    filename = f"{timestamp}_{unique_id}.pdf"
                
                # ç§»é™¤ä¸å®‰å…¨å­—ç¬¦
                filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
                filepath = os.path.join(save_dir, filename)

                # ä¿å­˜è‡³æœ¬åœ°
                with open(filepath, 'wb') as f:
                    f.write(response.data)
                print(f"ğŸ“„ PDFæ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")
                
                with pdfplumber.open(pdf_file) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text() + "\n"
                return f"ğŸ“„ PDFæ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}\nè®ºæ–‡å†…å®¹: {text}"
            elif attempt < max_retries - 1:
                time.sleep(2 ** (attempt + 2))
            else:
                raise Exception(f"ä¸‹è½½è®ºæ–‡æ—¶æ”¶åˆ°é2xxçŠ¶æ€ç : {response.status}")
    except Exception as e:
        return f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {e}"
