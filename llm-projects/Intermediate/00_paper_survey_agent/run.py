#!/usr/bin/env python3
"""
ç§‘å­¦è®ºæ–‡è°ƒç ”æ™ºèƒ½åŠ©æ‰‹ - å•æ–‡ä»¶å¯è¿è¡Œç‰ˆæœ¬

åŸºäºLangGraphæ„å»ºçš„ç§‘å­¦è®ºæ–‡è°ƒç ”å’Œåˆ†æå·¥å…·ï¼Œæ”¯æŒé€šè¿‡CORE APIæœç´¢è®ºæ–‡ã€
ä¸‹è½½PDFæ–‡ä»¶ã€æå–å†…å®¹å¹¶ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Šã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python run.py
    python run.py --L cn    # ä½¿ç”¨ä¸­æ–‡æç¤ºè¯
    python run.py --Language en  # ä½¿ç”¨è‹±æ–‡æç¤ºè¯

ç¯å¢ƒè¦æ±‚ï¼š
    - OPENAI_API_KEY: OpenAI APIå¯†é’¥
    - OPENAI_BASE_URL: OpenAI APIåŸºç¡€URL
    - DEFAULT_MODEL: é»˜è®¤æ¨¡å‹
    - CORE_API_KEY: CORE APIå¯†é’¥ (å¯åœ¨ https://core.ac.uk/services/api#form ç”³è¯·)
    - TEMPERATURE: æ¸©åº¦å‚æ•°
"""

from enum import Enum
import os
import io
import time
import urllib3
import argparse
from typing import Optional, Sequence, Annotated, ClassVar, Literal, Dict, Any
from typing_extensions import TypedDict
from dotenv import load_dotenv
from pydantic import BaseModel, Field
import pdfplumber
import requests

from langchain.chat_models import init_chat_model
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool, BaseTool
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

load_dotenv()

def parse_args():
    parser = argparse.ArgumentParser(description='ç§‘å­¦è®ºæ–‡è°ƒç ”æ™ºèƒ½åŠ©æ‰‹')
    parser.add_argument('-L', '--Language', 
                       choices=['cn', 'en'], 
                       default='cn',
                       help='æç¤ºè¯è¯­è¨€ (cn: ä¸­æ–‡, en: è‹±æ–‡)')
    return parser.parse_args()

args = parse_args()
LANGUAGE = args.Language

# åŠ¨æ€å¯¼å…¥æç¤ºè¯
def load_prompts(language='cn'):
    """æ ¹æ®è¯­è¨€åŠ¨æ€åŠ è½½æç¤ºè¯"""
    if language == 'en':
        from src.prompt.prompt_en import (
            decision_making_prompt, planning_prompt, 
            agent_prompt, judge_prompt
        )
    else:
        from src.prompt.prompt_cn import (
            decision_making_prompt, planning_prompt, 
            agent_prompt, judge_prompt
        )
    return decision_making_prompt, planning_prompt, agent_prompt, judge_prompt

decision_making_prompt, planning_prompt, agent_prompt, judge_prompt = load_prompts(LANGUAGE)

# å…¨å±€é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")
CORE_API_KEY = os.getenv("CORE_API_KEY")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL")
TEMPERATURE = os.getenv("TEMPERATURE")
MAX_RESEARCH_PAPERS = os.getenv("MAX_RESEARCH_PAPERS")

if not OPENAI_API_KEY:
    raise ValueError("è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
if not CORE_API_KEY:
    raise ValueError("è¯·è®¾ç½®CORE_API_KEYç¯å¢ƒå˜é‡")

# åˆå§‹åŒ–LLM
llm = init_chat_model(
    f"openai:{DEFAULT_MODEL}",
    temperature=TEMPERATURE,
    base_url=OPENAI_BASE_URL,
    api_key=OPENAI_API_KEY
)

# ========================= æ•°æ®æ¨¡å‹å®šä¹‰ =========================

class SearchPapersInput(BaseModel):
    """æœç´¢è®ºæ–‡çš„è¾“å…¥æ¨¡å‹"""
    query: str = Field(description="åœ¨é€‰å®šæ¡£æ¡ˆä¸­æœç´¢çš„æŸ¥è¯¢")
    max_papers: int = Field(
        description="è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ã€‚é»˜è®¤ä¸º1ï¼Œä½†å¦‚æœéœ€è¦æ›´å…¨é¢çš„æœç´¢ï¼Œå¯ä»¥å¢åŠ åˆ°100",
        default=1, ge=1, le=100
    )

class TypeEnum(str, Enum):
    usual = "usual"         # æ—¥å¸¸é—®ç­”
    search = "search"       # æœç´¢è®ºæ–‡
    download = "download"   # ä¸‹è½½è®ºæ–‡
    analyze = "analyze"     # åˆ†æè®ºæ–‡
    report = "report"       # ç”Ÿæˆç»¼è¿° / æŠ¥å‘Š

class DecisionMakingOutput(BaseModel):
    """å†³ç­–èŠ‚ç‚¹çš„è¾“å‡ºæ¨¡å‹"""
    requires_research: bool = Field(description="ç”¨æˆ·æŸ¥è¯¢æ˜¯å¦éœ€è¦ç ”ç©¶")
    type: TypeEnum = Field(description="ç”¨æˆ·æŸ¥è¯¢ç±»å‹")
    answer: Optional[str] = Field(
        default=None, 
        description="ç”¨æˆ·æŸ¥è¯¢çš„ç­”æ¡ˆã€‚å¦‚æœéœ€è¦ç ”ç©¶åˆ™ä¸ºNoneï¼Œå¦åˆ™ä¸ºç›´æ¥ç­”æ¡ˆ"
    )

class JudgeOutput(BaseModel):
    """åˆ¤æ–­èŠ‚ç‚¹çš„è¾“å‡ºæ¨¡å‹"""
    is_good_answer: bool = Field(description="ç­”æ¡ˆæ˜¯å¦è‰¯å¥½")
    feedback: Optional[str] = Field(
        default=None, 
        description="å…³äºç­”æ¡ˆä¸å¥½çš„è¯¦ç»†åé¦ˆã€‚å¦‚æœç­”æ¡ˆè‰¯å¥½åˆ™ä¸ºNone"
    )

class AgentState(TypedDict):
    """çŠ¶æ€å›¾"""
    requires_research: bool
    type: TypeEnum
    num_feedback_requests: int
    is_good_answer: bool
    messages: Annotated[Sequence[BaseMessage], add_messages]

# ========================= CORE APIå°è£… =========================

class CoreAPIWrapper(BaseModel):
    """CORE APIçš„ç®€å•å°è£…"""
    base_url: ClassVar[str] = "https://api.core.ac.uk/v3"
    api_key: ClassVar[str] = CORE_API_KEY
    top_k_results: int = 1

    def _get_search_response(self, query: str) -> Dict[str, Any]:
        """è·å–æœç´¢å“åº”"""
        url = f"{self.base_url}/search/works"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "q": query,
            "limit": self.top_k_results,
            "offset": 0,
            "scroll": False
        }
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(url, json=data, headers=headers, timeout=30)
                response.raise_for_status()
                return response.json()
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    raise Exception(f"CORE APIè¯·æ±‚å¤±è´¥: {e}")

    def search(self, query: str) -> str:
        """æœç´¢è®ºæ–‡"""
        response = self._get_search_response(query)
        results = response.get("results", [])
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"

        # æ ¼å¼åŒ–ç»“æœ
        docs = []
        for result in results:
            published_date_str = result.get('publishedDate') or result.get('yearPublished', '')
            authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])
            docs.append((
                f"* ID: {result.get('id', '')}\n"
                f"* æ ‡é¢˜: {result.get('title', '')}\n"
                f"* å‘è¡¨æ—¥æœŸ: {published_date_str}\n"
                f"* ä½œè€…: {authors_str}\n"
                f"* æ‘˜è¦: {result.get('abstract', '')}\n"
                f"* è®ºæ–‡é“¾æ¥: {result.get('sourceFulltextUrls') or result.get('downloadUrl', '')}"
            ))
        return "\n-----\n".join(docs)

# ========================= å·¥å…·å®šä¹‰ =========================

@tool("search-papers", args_schema=SearchPapersInput)
def search_papers(query: str, max_papers: int = 1) -> str:
    """ä½¿ç”¨CORE APIæœç´¢ç§‘å­¦è®ºæ–‡
    
    ç¤ºä¾‹ï¼š
    {"query": "Attention is all you need", "max_papers": 1}
    
    è¿”å›ï¼š
        æ‰¾åˆ°çš„ç›¸å…³è®ºæ–‡åˆ—è¡¨åŠå¯¹åº”çš„ç›¸å…³ä¿¡æ¯
    """
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢è®ºæ–‡: {query} (æœ€å¤š {max_papers} ç¯‡)")
        return CoreAPIWrapper(top_k_results=max_papers).search(query)
    except Exception as e:
        return f"æ‰§è¡Œè®ºæ–‡æœç´¢æ—¶å‡ºé”™: {e}"

@tool("download-paper")
def download_paper(url: str) -> str:
    """ä»ç»™å®šURLä¸‹è½½ç‰¹å®šç§‘å­¦è®ºæ–‡ï¼ˆè‹¥åªç»™å®šè®ºæ–‡æ ‡é¢˜ï¼Œè¯·å…ˆä½¿ç”¨ `search_papers` å·¥å…·è¿›è¡ŒæŸ¥è¯¢è®ºæ–‡ï¼Œè·å–ä¸‹è½½URLï¼‰
    
    ç¤ºä¾‹ï¼š
    {"url": "https://sample.pdf"}
    
    è¿”å›ï¼š
        è®ºæ–‡å†…å®¹
    """
    try:
        http = urllib3.PoolManager(cert_reqs='CERT_NONE')
        
        # æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´é¿å…403é”™è¯¯
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        max_retries = 5
        for attempt in range(max_retries):
            response = http.request('GET', url, headers=headers)
            if 200 <= response.status < 300:
                pdf_file = io.BytesIO(response.data)
                # TODOï¼šä¿å­˜PDFæ–‡ä»¶åˆ°è¿è¡Œç›®å½•
                with pdfplumber.open(pdf_file) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text() + "\n"
                return text
            elif attempt < max_retries - 1:
                time.sleep(2 ** (attempt + 2))
            else:
                raise Exception(f"ä¸‹è½½è®ºæ–‡æ—¶æ”¶åˆ°é2xxçŠ¶æ€ç : {response.status}")
    except Exception as e:
        return f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {e}"

@tool("ask-human-feedback")
def ask_human_feedback(question: str) -> str:
    """è¯¢é—®äººç±»åé¦ˆã€‚é‡åˆ°æ„å¤–é”™è¯¯æ—¶åº”è°ƒç”¨æ­¤å·¥å…·"""
    return input(f"[äººç±»åé¦ˆè¯·æ±‚] {question}: ")

tools = [search_papers, download_paper, ask_human_feedback]
tools_dict = {tool.name: tool for tool in tools}

# ========================= è¾…åŠ©å‡½æ•° =========================

def format_tools_description(tools: list[BaseTool]) -> str:
    """æ ¼å¼åŒ–å·¥å…·æè¿°"""
    return "\n\n".join([f"- {tool.name}: {tool.description}\nè¾“å…¥å‚æ•°: {tool.args}" for tool in tools])

# ========================= èŠ‚ç‚¹å®šä¹‰ =========================

# åˆå§‹åŒ–LLMå®ä¾‹
decision_making_llm = llm.with_structured_output(DecisionMakingOutput)
agent_llm = llm.bind_tools(tools)
judge_llm = llm.with_structured_output(JudgeOutput)

def decision_making_node(state: AgentState) -> Dict[str, Any]:
    """å†³ç­–èŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢å†³å®šæ˜¯å¦éœ€è¦ç ”ç©¶"""
    system_prompt = SystemMessage(content=decision_making_prompt)
    response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state["messages"])
    
    output = {"requires_research": response.requires_research, "type": response.type}
    if response.answer:
        output["messages"] = [AIMessage(content=response.answer)]
    return output

def router(state: AgentState) -> Literal["planning", "end"]:
    """è·¯ç”±å™¨ï¼šå°†ç”¨æˆ·æŸ¥è¯¢å¼•å¯¼åˆ°é€‚å½“çš„å·¥ä½œæµåˆ†æ”¯"""
    if state["requires_research"]:
        # print(state["type"]) # TypeEnum.search
        if state["type"] == TypeEnum.report:
            state["messages"][-1].content = f"æŸ¥è¯¢ {MAX_RESEARCH_PAPERS} ç¯‡èƒ½å¤Ÿè¾…åŠ©è®ºæ–‡ä¸»é¢˜ {state['messages'][-1].content} çš„ç›¸å…³å¯å¼•ç”¨è®ºæ–‡ã€‚\næ— è®ºè®ºæ–‡ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Œä½ çš„ä»»åŠ¡ä»…æœ‰æŸ¥è¯¢ã€‚"
        return "planning"
    else:
        return "end"

def planning_node(state: AgentState) -> Dict[str, Any]:
    """è§„åˆ’èŠ‚ç‚¹ï¼šåˆ›å»ºé€æ­¥è®¡åˆ’æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢"""
    system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))
    response = llm.invoke([system_prompt] + state["messages"])
    return {"messages": [response]}

def tools_node(state: AgentState) -> Dict[str, Any]:
    """å·¥å…·èŠ‚ç‚¹ï¼šåŸºäºè®¡åˆ’æ‰§è¡Œå·¥å…·"""
    outputs = []
    for tool_call in state["messages"][-1].tool_calls:
        tool_result = tools_dict[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=str(tool_result),
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}

def agent_node(state: AgentState) -> Dict[str, Any]:
    """æ™ºèƒ½åŠ©æ‰‹èŠ‚ç‚¹ï¼šä½¿ç”¨å¸¦å·¥å…·çš„LLMå›ç­”ç”¨æˆ·æŸ¥è¯¢"""
    system_prompt = SystemMessage(content=agent_prompt)
    response = agent_llm.invoke([system_prompt] + state["messages"])
    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["continue", "end"]:
    """æ£€æŸ¥æ™ºèƒ½åŠ©æ‰‹æ˜¯å¦åº”è¯¥ç»§ç»­æˆ–ç»“æŸ"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸæ‰§è¡Œ
    if last_message.tool_calls:
        return "continue"
    else:
        # if state["type"] == TypeEnum.report:
        #     return "generate_survey"
        return "end"

# def generate_survey_node(state: AgentState) -> Dict[str, Any]:
#     """ç”Ÿæˆç»¼è¿°èŠ‚ç‚¹ï¼šæ ¹æ®å·¥å…·è°ƒç”¨ç»“æœç”Ÿæˆç»¼è¿°"""
#     pass

def judge_node(state: AgentState) -> Dict[str, Any]:
    """åˆ¤æ–­èŠ‚ç‚¹ï¼šè®©LLMè¯„ä¼°è‡ªå·±æœ€ç»ˆç­”æ¡ˆçš„è´¨é‡"""
    num_feedback_requests = state.get("num_feedback_requests", 0)
    
    # å¦‚æœLLMä¸¤æ¬¡æœªèƒ½æä¾›è‰¯å¥½ç­”æ¡ˆï¼Œåˆ™ç»“æŸæ‰§è¡Œ
    if num_feedback_requests >= 2:
        return {"is_good_answer": True}
    
    system_prompt = SystemMessage(content=judge_prompt)
    response: JudgeOutput = judge_llm.invoke([system_prompt] + state["messages"])
    
    output = {
        "is_good_answer": response.is_good_answer,
        "num_feedback_requests": num_feedback_requests + 1
    }
    if response.feedback:
        output["messages"] = [AIMessage(content=response.feedback)]
    return output

def final_answer_router(state: AgentState) -> Literal["planning", "end"]:
    """æœ€ç»ˆç­”æ¡ˆè·¯ç”±å™¨ï¼šç»“æŸå·¥ä½œæµæˆ–æ”¹è¿›ç­”æ¡ˆ"""
    if state["is_good_answer"]:
        return "end"
    else:
        return "planning"

# ========================= å·¥ä½œæµå®šä¹‰ =========================
# çŠ¶æ€å›¾
workflow = StateGraph(AgentState)

# èŠ‚ç‚¹
workflow.add_node("decision_making", decision_making_node)
workflow.add_node("planning", planning_node)
workflow.add_node("tools", tools_node)
workflow.add_node("agent", agent_node)
workflow.add_node("judge", judge_node)
# workflow.add_node("generate_survey", generate_survey_node)

# è¾¹
workflow.add_edge(START, "decision_making")

# æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "decision_making",
    router,
    {
        "planning": "planning",
        "end": END,
    }
)
workflow.add_edge("planning", "agent")
workflow.add_edge("tools", "agent")
workflow.add_edge("generate_survey", "judge")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "tools",
        # "generate_survey": "generate_survey",
        "end": "judge",
    },
)
workflow.add_conditional_edges(
    "judge",
    final_answer_router,
    {
        "planning": "planning",
        "end": END,
    }
)

# ç¼–è¯‘
app = workflow.compile()
mermaid_code = app.get_graph(xray=True).draw_mermaid()
with open("graph_visualization.mmd", "w") as f:
    f.write(mermaid_code)
print("Mermaid code saved as graph_visualization.mmd")

# ========================= ä¸»å‡½æ•° =========================

def print_stream(user_input: str):
    """æ‰“å°æµå¼è¾“å‡º"""
    print("=" * 50)
    print("ğŸ”¬ ç§‘å­¦è®ºæ–‡è°ƒç ”åŠ©æ‰‹")
    print("=" * 50)
    print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
    print("-" * 50)
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "requires_research": False,
        "num_feedback_requests": 0,
        "is_good_answer": False,
        "messages": [HumanMessage(content=user_input)]
    }
    
    # æµå¼æ‰§è¡Œ
    final_message = None
    for step in app.stream(initial_state):
        for node_name, node_output in step.items():
            print(f"ğŸ”„ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
            if "messages" in node_output:
                for message in node_output["messages"]:
                    if isinstance(message, AIMessage):
                        print(f"ğŸ¤– AIå›ç­”: {message.content}")
                        final_message = message
                    elif isinstance(message, ToolMessage):
                        print(f"ğŸ› ï¸ å·¥å…·ç»“æœ: {message.name}")
                        content = message.content
                        print(f"   ç»“æœ: {content}")
            print("-" * 30)
    
    print("=" * 50)
    print("âœ… å¤„ç†å®Œæˆ")
    print("=" * 50)
    
    return final_message

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨ç§‘å­¦è®ºæ–‡è°ƒç ”æ™ºèƒ½åŠ©æ‰‹!")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\nè¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢: ").strip()
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
                break
            
            if not user_input:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æŸ¥è¯¢")
                continue
            
            print_stream(user_input)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºå·²ä¸­æ–­ï¼Œæ„Ÿè°¢ä½¿ç”¨ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜")

if __name__ == "__main__":
    main()
