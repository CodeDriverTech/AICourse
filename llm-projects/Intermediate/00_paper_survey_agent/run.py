#!/usr/bin/env python3
"""
ç§‘å­¦è®ºæ–‡è°ƒç ”æ™ºèƒ½åŠ©æ‰‹ - å•æ–‡ä»¶å¯è¿è¡Œç‰ˆæœ¬

åŸºäºLangGraphæ„å»ºçš„ç§‘å­¦è®ºæ–‡è°ƒç ”å’Œåˆ†æå·¥å…·ï¼Œæ”¯æŒé€šè¿‡CORE APIæœç´¢è®ºæ–‡ã€
ä¸‹è½½PDFæ–‡ä»¶ã€æå–å†…å®¹å¹¶ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Šã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python run.py
    python run.py --L cn    # ä½¿ç”¨ä¸­æ–‡æç¤ºè¯
    python run.py --Language en  # ä½¿ç”¨è‹±æ–‡æç¤ºè¯

ç¯å¢ƒè¦æ±‚ï¼š
    - OPENAI_API_KEY: OpenAI APIå¯†é’¥
    - OPENAI_BASE_URL: OpenAI APIåŸºç¡€URL
    - DEFAULT_MODEL: é»˜è®¤æ¨¡å‹
    - CORE_API_KEY: CORE APIå¯†é’¥ (å¯åœ¨ https://core.ac.uk/services/api#form ç”³è¯·)
    - TEMPERATURE: æ¸©åº¦å‚æ•°
"""

from enum import Enum
import os
import io
import time
import urllib3
import argparse
from datetime import datetime
from typing import Optional, Sequence, Annotated, ClassVar, Literal, Dict, Any
from typing_extensions import TypedDict
from dotenv import load_dotenv
from pydantic import BaseModel, Field
import pdfplumber
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

from langchain.chat_models import init_chat_model
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool, BaseTool
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

load_dotenv()

def parse_args():
    parser = argparse.ArgumentParser(description='ç§‘å­¦è®ºæ–‡è°ƒç ”æ™ºèƒ½åŠ©æ‰‹')
    parser.add_argument('-L', '--Language', 
                       choices=['cn', 'en'], 
                       default='cn',
                       help='æç¤ºè¯è¯­è¨€ (cn: ä¸­æ–‡, en: è‹±æ–‡)')
    return parser.parse_args()

args = parse_args()
LANGUAGE = args.Language

# åŠ¨æ€å¯¼å…¥æç¤ºè¯
def load_prompts(language='cn'):
    """æ ¹æ®è¯­è¨€åŠ¨æ€åŠ è½½æç¤ºè¯"""
    if language == 'en':
        from src.prompt.prompt_en import (
            decision_making_prompt, planning_prompt, 
            agent_prompt, judge_prompt,
            paper_analysis_prompt, survey_outline_prompt,
            survey_section_prompt, survey_title_abstract_prompt
        )
    else:
        from src.prompt.prompt_cn import (
            decision_making_prompt, planning_prompt, 
            agent_prompt, judge_prompt,
            paper_analysis_prompt, survey_outline_prompt,
            survey_section_prompt, survey_title_abstract_prompt
        )
    return (decision_making_prompt, planning_prompt, agent_prompt, judge_prompt,
            paper_analysis_prompt, survey_outline_prompt,
            survey_section_prompt, survey_title_abstract_prompt)

(decision_making_prompt, planning_prompt, agent_prompt, judge_prompt,
 paper_analysis_prompt, survey_outline_prompt,
 survey_section_prompt, survey_title_abstract_prompt) = load_prompts(LANGUAGE)

# å…¨å±€é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")
CORE_API_KEY = os.getenv("CORE_API_KEY")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL")
TEMPERATURE = os.getenv("TEMPERATURE")
MAX_SURVEY_REFERENCE = os.getenv("MAX_SURVEY_REFERENCE")
SAVE_DIR = os.getenv("SAVE_DIR")

if not OPENAI_API_KEY:
    raise ValueError("è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
if not CORE_API_KEY:
    raise ValueError("è¯·è®¾ç½®CORE_API_KEYç¯å¢ƒå˜é‡")

# åˆå§‹åŒ–LLM
llm = init_chat_model(
    f"openai:{DEFAULT_MODEL}",
    temperature=TEMPERATURE,
    base_url=OPENAI_BASE_URL,
    api_key=OPENAI_API_KEY
)

# ========================= æ•°æ®æ¨¡å‹å®šä¹‰ =========================

class SearchPapersInput(BaseModel):
    """æœç´¢è®ºæ–‡çš„è¾“å…¥æ¨¡å‹"""
    query: str = Field(description="åœ¨é€‰å®šæ¡£æ¡ˆä¸­æœç´¢çš„æŸ¥è¯¢")
    max_papers: int = Field(
        description="è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ã€‚é»˜è®¤ä¸º1ï¼Œä½†å¦‚æœéœ€è¦æ›´å…¨é¢çš„æœç´¢ï¼Œå¯ä»¥å¢åŠ åˆ°100",
        default=1, ge=1, le=100
    )

class TypeEnum(str, Enum):
    usual = "usual"         # æ—¥å¸¸é—®ç­”
    search = "search"       # æœç´¢è®ºæ–‡
    download = "download"   # ä¸‹è½½è®ºæ–‡
    analyze = "analyze"     # åˆ†æè®ºæ–‡
    report = "report"       # ç”Ÿæˆç»¼è¿° / æŠ¥å‘Š

class DecisionMakingOutput(BaseModel):
    """å†³ç­–èŠ‚ç‚¹çš„è¾“å‡ºæ¨¡å‹"""
    requires_research: bool = Field(description="ç”¨æˆ·æŸ¥è¯¢æ˜¯å¦éœ€è¦ç ”ç©¶")
    type: TypeEnum = Field(description="ç”¨æˆ·æŸ¥è¯¢ç±»å‹")
    answer: Optional[str] = Field(
        default=None, 
        description="ç”¨æˆ·æŸ¥è¯¢çš„ç­”æ¡ˆã€‚å¦‚æœéœ€è¦ç ”ç©¶åˆ™ä¸ºNoneï¼Œå¦åˆ™ä¸ºç›´æ¥ç­”æ¡ˆ"
    )

class JudgeOutput(BaseModel):
    """åˆ¤æ–­èŠ‚ç‚¹çš„è¾“å‡ºæ¨¡å‹"""
    is_good_answer: bool = Field(description="ç­”æ¡ˆæ˜¯å¦è‰¯å¥½")
    feedback: Optional[str] = Field(
        default=None, 
        description="å…³äºç­”æ¡ˆä¸å¥½çš„è¯¦ç»†åé¦ˆã€‚å¦‚æœç­”æ¡ˆè‰¯å¥½åˆ™ä¸ºNone"
    )

class AgentState(TypedDict):
    """çŠ¶æ€å›¾"""
    requires_research: bool
    type: TypeEnum
    num_feedback_requests: int
    is_good_answer: bool
    messages: Annotated[Sequence[BaseMessage], add_messages]

# ========================= CORE APIå°è£… =========================

class CoreAPIWrapper(BaseModel):
    """CORE APIçš„ç®€å•å°è£…"""
    base_url: ClassVar[str] = "https://api.core.ac.uk/v3"
    api_key: ClassVar[str] = CORE_API_KEY
    top_k_results: int = 1

    def _get_search_response(self, query: str) -> Dict[str, Any]:
        """è·å–æœç´¢å“åº”"""
        url = f"{self.base_url}/search/works"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "q": query,
            "limit": self.top_k_results,
            "offset": 0,
            "scroll": False
        }
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(url, json=data, headers=headers, timeout=30)
                response.raise_for_status()
                original_output = response.json()
                
                # å¹¶è¡Œä¸‹è½½è®ºæ–‡
                download_urls = [result.get("downloadUrl") for result in original_output.get("results", []) if result.get("downloadUrl")]
                if download_urls:
                    print(f"ğŸš€ å¼€å§‹å¹¶è¡Œä¸‹è½½ {len(download_urls)} ç¯‡è®ºæ–‡...")
                    self._parallel_download_papers(download_urls)
                
                return original_output
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    raise Exception(f"CORE APIè¯·æ±‚å¤±è´¥: {e}")
    
    def _parallel_download_papers(self, download_urls: list[str], max_workers: int = 3) -> None:
        """
        å¹¶è¡Œä¸‹è½½å¤šç¯‡è®ºæ–‡
        
        Args:
            download_urls: ä¸‹è½½é“¾æ¥åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º3
        """
        def download_single_paper(url: str) -> tuple[str, bool, str]:
            """ä¸‹è½½å•ç¯‡è®ºæ–‡çš„åŒ…è£…å‡½æ•°"""
            try:
                result = download_paper(url)
                return url, True, result
            except Exception as e:
                return url, False, str(e)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(download_single_paper, url): url for url in download_urls}
            
            completed_count = 0
            failed_count = 0
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    original_url, success, result = future.result()
                    if success:
                        completed_count += 1
                        print(f"  âœ… ä¸‹è½½å®Œæˆ ({completed_count}/{len(download_urls)}): {os.path.basename(original_url)}")
                    else:
                        failed_count += 1
                        print(f"  âŒ ä¸‹è½½å¤±è´¥ ({failed_count}/{len(download_urls)}): {result}")
                except Exception as e:
                    failed_count += 1
                    print(f"  âŒ ä¸‹è½½å¼‚å¸¸ ({failed_count}/{len(download_urls)}): {e}")
            
            print(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡: æˆåŠŸ {completed_count} ç¯‡, å¤±è´¥ {failed_count} ç¯‡")

    def search(self, query: str) -> str:
        """æœç´¢è®ºæ–‡"""
        response = self._get_search_response(query)
        results = response.get("results", [])
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"

        # æ ¼å¼åŒ–ç»“æœ
        docs = []
        for result in results:
            published_date_str = result.get('publishedDate') or result.get('yearPublished', '')
            authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])
            docs.append((
                f"* ID: {result.get('id', '')}\n"
                f"* æ ‡é¢˜: {result.get('title', '')}\n"
                f"* å‘è¡¨æ—¥æœŸ: {published_date_str}\n"
                f"* ä½œè€…: {authors_str}\n"
                f"* æ‘˜è¦: {result.get('abstract', '')}\n"
                f"* è®ºæ–‡ä¸‹è½½é“¾æ¥: {result.get('downloadUrl') or result.get('sourceFulltextUrls', '')}"
            ))
        return "\n-----\n".join(docs)

# ========================= å·¥å…·å®šä¹‰ =========================

@tool("search-papers", args_schema=SearchPapersInput)
def search_papers(query: str, max_papers: int = 1) -> str:
    """ä½¿ç”¨CORE APIæœç´¢ç§‘å­¦è®ºæ–‡
    
    ç¤ºä¾‹ï¼š
    {"query": "Attention is all you need", "max_papers": 1}
    
    è¿”å›ï¼š
        æ‰¾åˆ°çš„ç›¸å…³è®ºæ–‡åˆ—è¡¨åŠå¯¹åº”çš„ç›¸å…³ä¿¡æ¯
    """
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢è®ºæ–‡: {query} (æœ€å¤š {max_papers} ç¯‡)")
        return CoreAPIWrapper(top_k_results=max_papers).search(query)
    except Exception as e:
        return f"æ‰§è¡Œè®ºæ–‡æœç´¢æ—¶å‡ºé”™: {e}"

@tool("download-paper")
def download_paper(url: str) -> str:
    """ä»ç»™å®šURLä¸‹è½½ç‰¹å®šç§‘å­¦è®ºæ–‡ï¼ˆè‹¥åªç»™å®šè®ºæ–‡æ ‡é¢˜ï¼Œè¯·å…ˆä½¿ç”¨ `search_papers` å·¥å…·è¿›è¡ŒæŸ¥è¯¢è®ºæ–‡ï¼Œè·å–ä¸‹è½½URLï¼‰
    
    ç¤ºä¾‹ï¼š
    {"url": "https://sample.pdf"}
    
    è¿”å›ï¼š
        è®ºæ–‡å†…å®¹
    """
    try:
        http = urllib3.PoolManager(cert_reqs='CERT_NONE')
        
        # æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´é¿å…403é”™è¯¯
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        max_retries = 5
        for attempt in range(max_retries):
            response = http.request('GET', url, headers=headers)
            if 200 <= response.status < 300:
                pdf_file = io.BytesIO(response.data)

                save_dir = SAVE_DIR
                os.makedirs(save_dir, exist_ok=True)

                import re
                from urllib.parse import urlparse

                parsed_url = urlparse(url)
                filename = os.path.basename(parsed_url.path)
                if not filename.endswith('.pdf'):
                    # å¦‚æœæ— æ³•ä»URLè·å–æ–‡ä»¶åï¼Œä½¿ç”¨æ—¶é—´æˆ³
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"paper_{timestamp}.pdf"
                
                # ç§»é™¤ä¸å®‰å…¨å­—ç¬¦
                filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
                filepath = os.path.join(save_dir, filename)

                # ä¿å­˜è‡³æœ¬åœ°
                with open(filepath, 'wb') as f:
                    f.write(response.data)
                print(f"ğŸ“„ PDFæ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")
                
                with pdfplumber.open(pdf_file) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text() + "\n"
                return f"ğŸ“„ PDFæ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}\nè®ºæ–‡å†…å®¹: {text}"
            elif attempt < max_retries - 1:
                time.sleep(2 ** (attempt + 2))
            else:
                raise Exception(f"ä¸‹è½½è®ºæ–‡æ—¶æ”¶åˆ°é2xxçŠ¶æ€ç : {response.status}")
    except Exception as e:
        return f"ä¸‹è½½è®ºæ–‡æ—¶å‡ºé”™: {e}"

@tool("ask-human-feedback")
def ask_human_feedback(question: str) -> str:
    """è¯¢é—®äººç±»åé¦ˆã€‚é‡åˆ°æ„å¤–é”™è¯¯æ—¶åº”è°ƒç”¨æ­¤å·¥å…·"""
    return input(f"[äººç±»åé¦ˆè¯·æ±‚] {question}: ")

tools = [search_papers, download_paper, ask_human_feedback]
tools_dict = {tool.name: tool for tool in tools}

# ========================= è¾…åŠ©å‡½æ•° =========================

def format_tools_description(tools: list[BaseTool]) -> str:
    """æ ¼å¼åŒ–å·¥å…·æè¿°"""
    return "\n\n".join([f"- {tool.name}: {tool.description}\nè¾“å…¥å‚æ•°: {tool.args}" for tool in tools])

# ========================= èŠ‚ç‚¹å®šä¹‰ =========================

# åˆå§‹åŒ–LLMå®ä¾‹
decision_making_llm = llm.with_structured_output(DecisionMakingOutput)
agent_llm = llm.bind_tools(tools)
judge_llm = llm.with_structured_output(JudgeOutput)

def decision_making_node(state: AgentState) -> Dict[str, Any]:
    """å†³ç­–èŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢å†³å®šæ˜¯å¦éœ€è¦ç ”ç©¶"""
    system_prompt = SystemMessage(content=decision_making_prompt)
    response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state["messages"])
    
    output = {"requires_research": response.requires_research, "type": response.type}
    if response.answer:
        output["messages"] = [AIMessage(content=response.answer)]
    return output

def router(state: AgentState) -> Literal["planning", "end"]:
    """è·¯ç”±å™¨ï¼šå°†ç”¨æˆ·æŸ¥è¯¢å¼•å¯¼åˆ°é€‚å½“çš„å·¥ä½œæµåˆ†æ”¯"""
    if state["requires_research"]:
        # print(state["type"]) # TypeEnum.search
        if state["type"] == TypeEnum.report:
            state["messages"][-1].content = f"æŸ¥è¯¢ {MAX_SURVEY_REFERENCE} ç¯‡èƒ½å¤Ÿè¾…åŠ©è®ºæ–‡ä¸»é¢˜ {state['messages'][-1].content} çš„ç›¸å…³å¯å¼•ç”¨è®ºæ–‡ã€‚\næ— è®ºè®ºæ–‡ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Œä½ çš„ä»»åŠ¡ä»…æœ‰æŸ¥è¯¢ã€‚"
        return "planning"
    else:
        return "end"

def planning_node(state: AgentState) -> Dict[str, Any]:
    """è§„åˆ’èŠ‚ç‚¹ï¼šåˆ›å»ºé€æ­¥è®¡åˆ’æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢"""
    system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))
    response = llm.invoke([system_prompt] + state["messages"])
    return {"messages": [response]}

def tools_node(state: AgentState) -> Dict[str, Any]:
    """å·¥å…·èŠ‚ç‚¹ï¼šåŸºäºè®¡åˆ’æ‰§è¡Œå·¥å…·"""
    outputs = []
    for tool_call in state["messages"][-1].tool_calls:
        tool_result = tools_dict[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=str(tool_result),
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}

def agent_node(state: AgentState) -> Dict[str, Any]:
    """æ™ºèƒ½åŠ©æ‰‹èŠ‚ç‚¹ï¼šä½¿ç”¨å¸¦å·¥å…·çš„LLMå›ç­”ç”¨æˆ·æŸ¥è¯¢"""
    system_prompt = SystemMessage(content=agent_prompt)
    response = agent_llm.invoke([system_prompt] + state["messages"])
    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["continue", "end"]:
    """æ£€æŸ¥æ™ºèƒ½åŠ©æ‰‹æ˜¯å¦åº”è¯¥ç»§ç»­æˆ–ç»“æŸ"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸæ‰§è¡Œ
    if last_message.tool_calls:
        return "continue"
    else:
        if state["type"] == TypeEnum.report:
            state["messages"][0].content = f"{state['messages'][0].content}".replace(f"æŸ¥è¯¢ {MAX_SURVEY_REFERENCE} ç¯‡èƒ½å¤Ÿè¾…åŠ©è®ºæ–‡ä¸»é¢˜", "").replace("çš„ç›¸å…³å¯å¼•ç”¨è®ºæ–‡ã€‚\næ— è®ºè®ºæ–‡ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Œä½ çš„ä»»åŠ¡ä»…æœ‰æŸ¥è¯¢ã€‚", "")
            return "generate_survey"
        return "end"

# ========================= ç»¼è¿°ç”Ÿæˆç›¸å…³æ¨¡å‹ =========================

class SurveySection(BaseModel):
    """ç»¼è¿°ç« èŠ‚æ¨¡å‹"""
    title: str = Field(description="ç« èŠ‚æ ‡é¢˜")
    description: str = Field(description="ç« èŠ‚æè¿°å’Œè¦è¦†ç›–çš„ä¸»è¦å†…å®¹")
    priority: int = Field(description="ç« èŠ‚ä¼˜å…ˆçº§ (1-5, 5ä¸ºæœ€é«˜)", default=3)

class SurveySections(BaseModel):
    """ç»¼è¿°ç« èŠ‚åˆ—è¡¨æ¨¡å‹"""
    sections: list[SurveySection] = Field(description="ç»¼è¿°çš„æ‰€æœ‰ç« èŠ‚")

class PaperSummary(BaseModel):
    """è®ºæ–‡æ‘˜è¦æ¨¡å‹"""
    title: str = Field(description="è®ºæ–‡æ ‡é¢˜")
    abstract: str = Field(description="è®ºæ–‡æ‘˜è¦")
    key_contributions: list[str] = Field(description="è®ºæ–‡çš„å…³é”®è´¡çŒ®ç‚¹")
    methodology: str = Field(description="ç ”ç©¶æ–¹æ³•")
    limitations: str = Field(description="ç ”ç©¶å±€é™æ€§")
    relevance_score: int = Field(description="ä¸ç»¼è¿°ä¸»é¢˜çš„ç›¸å…³æ€§è¯„åˆ† (1-10)", default=5)

# ========================= PDFæ‰¹é‡è¯»å–å·¥å…· =========================

def extract_paper_content_from_save_dir() -> list[dict]:
    """ä»ä¿å­˜ç›®å½•ä¸­æå–å·²ä¸‹è½½çš„è®ºæ–‡ä¿¡æ¯"""
    papers_info = []

    if not SAVE_DIR or not os.path.exists(SAVE_DIR):
        print(f"âš ï¸ ä¿å­˜ç›®å½• {SAVE_DIR} ä¸å­˜åœ¨")
        return papers_info

    try:
        for filename in os.listdir(SAVE_DIR):
            if filename.lower().endswith('.pdf'):
                filepath = os.path.join(SAVE_DIR, filename)
                print(f"ğŸ“„ è¯»å–PDFæ–‡ä»¶: {filename}")
                
                try:
                    with pdfplumber.open(filepath) as pdf:
                        text_content = ""
                        for page in pdf.pages:
                            page_text = page.extract_text()
                            if page_text:
                                text_content += page_text + "\n"
                        
                        if text_content.strip():
                            papers_info.append({
                                'filepath': filepath,
                                'filename': filename,
                                'content': text_content.strip()
                            })
                            print(f"  âœ… æˆåŠŸè¯»å–ï¼Œå†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
                        else:
                            print(f"  âš ï¸ PDFæ–‡ä»¶ {filename} å†…å®¹ä¸ºç©º")
                            
                except Exception as e:
                    print(f"  âŒ è¯»å–PDFæ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    
    except Exception as e:
        print(f"âŒ éå†ä¿å­˜ç›®å½•å¤±è´¥: {e}")
    
    return papers_info

def summarize_paper_content(paper_content: str, topic: str) -> PaperSummary:
    """ä½¿ç”¨LLMæ€»ç»“è®ºæ–‡å†…å®¹"""
    summarize_llm = llm.with_structured_output(PaperSummary)

    prompt = paper_analysis_prompt.format(
        topic=topic,
        paper_content=paper_content[:4000]
    )
    
    try:
        summary = summarize_llm.invoke([HumanMessage(content=prompt)])
        return summary
    except Exception as e:
        # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
        return PaperSummary(
            title="æœªèƒ½æå–æ ‡é¢˜",
            abstract="è®ºæ–‡å†…å®¹è§£æå¤±è´¥",
            key_contributions=["è§£æå¤±è´¥"],
            methodology="æœªçŸ¥",
            limitations="æœªçŸ¥",
            relevance_score=1
        )

# ========================= ç»¼è¿°ç”Ÿæˆæ ¸å¿ƒé€»è¾‘ =========================

def _parallel_generate_sections(sections: list, topic: str, paper_summaries: list, max_workers: int = 2) -> list[str]:
    """
    å¹¶è¡Œç”Ÿæˆç»¼è¿°ç« èŠ‚å†…å®¹ï¼Œä¿è¯ç« èŠ‚é¡ºåº
    
    Args:
        sections: ç« èŠ‚åˆ—è¡¨
        topic: ç»¼è¿°ä¸»é¢˜
        paper_summaries: è®ºæ–‡æ‘˜è¦åˆ—è¡¨
        max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º2ï¼ˆé¿å…APIé™åˆ¶ï¼‰
    
    Returns:
        æŒ‰åŸå§‹é¡ºåºæ’åˆ—çš„å®Œæ•´ç« èŠ‚å†…å®¹åˆ—è¡¨
    """
    def generate_single_section(section_data: tuple) -> tuple[int, str]:
        """ç”Ÿæˆå•ä¸ªç« èŠ‚å†…å®¹çš„åŒ…è£…å‡½æ•°"""
        index, section = section_data
        try:
            print(f"  ğŸ“ å¼€å§‹ç”Ÿæˆç¬¬ {index+1} ç« ï¼š{section.title}")
            
            section_prompt_text = survey_section_prompt.format(
                section_title=section.title,
                section_description=section.description,
                topic=topic,
                paper_info=chr(10).join([f"è®ºæ–‡{j+1}: {summary.title}\nå…³é”®è´¡çŒ®: {', '.join(summary.key_contributions)}\næ–¹æ³•: {summary.methodology}\n" for j, summary in enumerate(paper_summaries)])
            )
            
            section_content = llm.invoke([HumanMessage(content=section_prompt_text)])
            completed_section = f"## {section.title}\n\n{section_content.content}"
            
            print(f"  âœ… å®Œæˆç¬¬ {index+1} ç« ï¼š{section.title}")
            return index, completed_section
            
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆç¬¬ {index+1} ç« å¤±è´¥ï¼š{e}")
            return index, f"## {section.title}\n\nç”Ÿæˆå¤±è´¥ï¼š{str(e)}"
    
    # åˆ›å»ºå¸¦ç´¢å¼•çš„ç« èŠ‚åˆ—è¡¨
    indexed_sections = [(i, section) for i, section in enumerate(sections)]
    completed_sections = [None] * len(sections)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_index = {executor.submit(generate_single_section, section_data): section_data[0] 
                          for section_data in indexed_sections}
        
        completed_count = 0

        for future in as_completed(future_to_index):
            index = future_to_index[future]
            try:
                section_index, section_content = future.result()
                completed_sections[section_index] = section_content
                completed_count += 1
                print(f"    ğŸ¯ ç« èŠ‚ç”Ÿæˆè¿›åº¦: {completed_count}/{len(sections)}")
            except Exception as e:
                print(f"    âŒ å¤„ç†ç¬¬ {index+1} ç« ç»“æœæ—¶å‡ºé”™ï¼š{e}")
                completed_sections[index] = f"## ç¬¬{index+1}ç« \n\nç”Ÿæˆå¼‚å¸¸ï¼š{str(e)}"
    
    print(f"ğŸ“Š ç« èŠ‚ç”Ÿæˆç»Ÿè®¡: å®Œæˆ {len([s for s in completed_sections if s])} / {len(sections)} ç« ")

    return [section for section in completed_sections if section]

def generate_survey_agent(state: AgentState) -> Dict[str, Any]:
    """ç”Ÿæˆè®ºæ–‡ç»¼è¿°èŠ‚ç‚¹ï¼šåŸºäºåè°ƒè€…-å·¥ä½œè€…æ¨¡å¼ç”Ÿæˆé«˜è´¨é‡ç»¼è¿°æŠ¥å‘Šï¼ˆåŸºæœ¬å®ç°æ€è·¯å¯ä»¥å‚è€ƒ `./llm-projects/Beginner/02_building_effective_agents/4_orchestrator-worker/graph_api.py`ï¼‰"""
    try:
        # 1. æå–åŸå§‹ç”¨æˆ·æŸ¥è¯¢ï¼ˆç»¼è¿°ä¸»é¢˜ï¼‰
        original_query = state["messages"][0].content
        print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆç»¼è¿°æŠ¥å‘Šï¼Œä¸»é¢˜ï¼š{original_query}")
        
        # 2. ä»ä¿å­˜ç›®å½•ä¸­æå–å·²ä¸‹è½½çš„è®ºæ–‡ä¿¡æ¯
        papers_info = extract_paper_content_from_save_dir()
        print(f"ğŸ“š ä» {SAVE_DIR} ç›®å½•å‘ç° {len(papers_info)} ç¯‡å·²ä¸‹è½½çš„è®ºæ–‡")
        
        if not papers_info:
            return {
                "messages": [AIMessage(content="âš ï¸ æœªæ‰¾åˆ°å·²ä¸‹è½½çš„è®ºæ–‡ï¼Œæ— æ³•ç”Ÿæˆç»¼è¿°æŠ¥å‘Šã€‚è¯·å…ˆæœç´¢å’Œä¸‹è½½ç›¸å…³è®ºæ–‡ã€‚")]
            }
        
        # 3. åè°ƒè€…é˜¶æ®µï¼šåˆ†æè®ºæ–‡å¹¶è§„åˆ’ç»¼è¿°ç»“æ„
        print("ğŸ§  åè°ƒè€…é˜¶æ®µï¼šåˆ†æè®ºæ–‡å†…å®¹å¹¶è§„åˆ’ç»¼è¿°ç»“æ„...")
        paper_summaries = []
        for i, paper in enumerate(papers_info):
            print(f"  ğŸ“„ åˆ†æç¬¬ {i+1} ç¯‡è®ºæ–‡...")
            summary = summarize_paper_content(paper['content'], original_query)
            paper_summaries.append(summary)
        
        # 4. ç”Ÿæˆç»¼è¿°å¤§çº²
        planner_llm = llm.with_structured_output(SurveySections)
        outline_prompt_text = survey_outline_prompt.format(
            topic=original_query,
            paper_summaries=chr(10).join([f"è®ºæ–‡{i+1}: {summary.title} - {summary.abstract[:200]}..." for i, summary in enumerate(paper_summaries)])
        )
        
        survey_outline = planner_llm.invoke([HumanMessage(content=outline_prompt_text)])
        print(f"ğŸ“‹ ç”Ÿæˆäº† {len(survey_outline.sections)} ä¸ªç»¼è¿°ç« èŠ‚")

        print("âœï¸ å·¥ä½œè€…é˜¶æ®µï¼šå¹¶è¡Œç”Ÿæˆå„ç« èŠ‚è¯¦ç»†å†…å®¹...")
        
        # å¹¶è¡Œç”Ÿæˆç« èŠ‚å†…å®¹
        completed_sections = _parallel_generate_sections(
            survey_outline.sections, 
            original_query, 
            paper_summaries
        )
        
        # 6. åˆæˆå™¨é˜¶æ®µï¼šæ•´åˆæ‰€æœ‰ç« èŠ‚ç”Ÿæˆå®Œæ•´ç»¼è¿°
        print("ğŸ”— åˆæˆå™¨é˜¶æ®µï¼šæ•´åˆå®Œæ•´ç»¼è¿°æŠ¥å‘Š...")
        
        title_abstract_prompt_text = survey_title_abstract_prompt.format(
            topic=original_query,
            paper_count=len(paper_summaries)
        )
        
        title_abstract = llm.invoke([HumanMessage(content=title_abstract_prompt_text)])
        
        # æ•´åˆå®Œæ•´ç»¼è¿°
        full_survey = f"""
{title_abstract.content}

---

{chr(10).join(completed_sections)}

---

## å‚è€ƒæ–‡çŒ®

{chr(10).join([f"{i+1}. {summary.title}" for i, summary in enumerate(paper_summaries)])}

---

*æœ¬ç»¼è¿°åŸºäº {len(paper_summaries)} ç¯‡ç›¸å…³è®ºæ–‡ç”Ÿæˆï¼Œç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
        """
        
        # 7. ä¿å­˜ç»¼è¿°æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        survey_filename = f"survey_report_{timestamp}.md"
        
        try:
            with open(survey_filename, 'w', encoding='utf-8') as f:
                f.write(full_survey)
            print(f"ğŸ’¾ ç»¼è¿°æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{survey_filename}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»¼è¿°æŠ¥å‘Šæ—¶å‡ºé”™ï¼š{e}")
        
        # 8. è¿”å›ç»“æœ
        success_message = f"""
âœ… **ç»¼è¿°æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼**

ğŸ“Š **ç»Ÿè®¡ä¿¡æ¯ï¼š**
- åˆ†æè®ºæ–‡æ•°é‡ï¼š{len(paper_summaries)} ç¯‡
- ç»¼è¿°ç« èŠ‚æ•°é‡ï¼š{len(survey_outline.sections)} ä¸ª
- æŠ¥å‘Šä¿å­˜ä½ç½®ï¼š{survey_filename}

ğŸ“ **ç»¼è¿°é¢„è§ˆï¼š**
{full_survey[:1000]}...

ğŸ’¡ å®Œæ•´ç»¼è¿°æŠ¥å‘Šå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥è¿›ä¸€æ­¥ç¼–è¾‘å’Œå®Œå–„ã€‚
        """
        
        return {
            "messages": [AIMessage(content=success_message)]
        }
        
    except Exception as e:
        error_message = f"âŒ ç»¼è¿°ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}\n\nè¯·æ£€æŸ¥è®ºæ–‡ä¸‹è½½çŠ¶æ€å’Œç½‘ç»œè¿æ¥ã€‚"
        return {
            "messages": [AIMessage(content=error_message)]
        }

def judge_node(state: AgentState) -> Dict[str, Any]:
    """åˆ¤æ–­èŠ‚ç‚¹ï¼šè®©LLMè¯„ä¼°è‡ªå·±æœ€ç»ˆç­”æ¡ˆçš„è´¨é‡"""
    num_feedback_requests = state.get("num_feedback_requests", 0)
    
    # å¦‚æœLLMä¸‰æ¬¡æœªèƒ½æä¾›è‰¯å¥½ç­”æ¡ˆï¼Œåˆ™ç»“æŸæ‰§è¡Œ
    if num_feedback_requests >= 3:
        return {"is_good_answer": True}
    
    system_prompt = SystemMessage(content=judge_prompt)
    response: JudgeOutput = judge_llm.invoke([system_prompt] + state["messages"])
    
    output = {
        "is_good_answer": response.is_good_answer,
        "num_feedback_requests": num_feedback_requests + 1
    }
    if response.feedback:
        output["messages"] = [AIMessage(content=response.feedback)]
    return output

def final_answer_router(state: AgentState) -> Literal["planning", "end"]:
    """æœ€ç»ˆç­”æ¡ˆè·¯ç”±å™¨ï¼šç»“æŸå·¥ä½œæµæˆ–æ”¹è¿›ç­”æ¡ˆ"""
    if state["is_good_answer"]:
        return "end"
    else:
        return "planning"

# ========================= å·¥ä½œæµå®šä¹‰ =========================
# çŠ¶æ€å›¾
workflow = StateGraph(AgentState)

# èŠ‚ç‚¹
workflow.add_node("decision_making", decision_making_node)
workflow.add_node("planning", planning_node)
workflow.add_node("tools", tools_node)
workflow.add_node("agent", agent_node)
workflow.add_node("judge", judge_node)
# å°† Agent ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„èŠ‚ç‚¹
workflow.add_node("generate_survey", generate_survey_agent)

# è¾¹
workflow.add_edge(START, "decision_making")

# æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "decision_making",
    router,
    {
        "planning": "planning",
        "end": END,
    }
)
workflow.add_edge("planning", "agent")
workflow.add_edge("tools", "agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "tools",
        "generate_survey": "generate_survey",
        "end": "judge",
    },
)
workflow.add_edge("generate_survey", "judge")
workflow.add_conditional_edges(
    "judge",
    final_answer_router,
    {
        "planning": "planning",
        "end": END,
    }
)

# ç¼–è¯‘
app = workflow.compile()
mermaid_code = app.get_graph(xray=True).draw_mermaid()
with open("graph_visualization.mmd", "w") as f:
    f.write(mermaid_code)
print("Mermaid code saved as graph_visualization.mmd")

# ========================= ä¸»å‡½æ•° =========================

def print_stream(user_input: str):
    """æ‰“å°æµå¼è¾“å‡º"""
    print("=" * 50)
    print("ğŸ”¬ ç§‘å­¦è®ºæ–‡è°ƒç ”åŠ©æ‰‹")
    print("=" * 50)
    print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
    print("-" * 50)
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "requires_research": False,
        "num_feedback_requests": 0,
        "is_good_answer": False,
        "messages": [HumanMessage(content=user_input)]
    }
    
    # æµå¼æ‰§è¡Œ
    final_message = None
    for step in app.stream(initial_state):
        for node_name, node_output in step.items():
            print(f"ğŸ”„ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
            if "messages" in node_output:
                for message in node_output["messages"]:
                    if isinstance(message, AIMessage):
                        print(f"ğŸ¤– AIå›ç­”: {message.content}")
                        final_message = message
                    elif isinstance(message, ToolMessage):
                        print(f"ğŸ› ï¸ å·¥å…·ç»“æœ: {message.name}")
                        content = message.content
                        print(f"   ç»“æœ: {content}")
            print("-" * 30)
    
    print("=" * 50)
    print("âœ… å¤„ç†å®Œæˆ")
    print("=" * 50)
    
    return final_message

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨ç§‘å­¦è®ºæ–‡è°ƒç ”æ™ºèƒ½åŠ©æ‰‹!")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\nè¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢: ").strip()
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
                break
            
            if not user_input:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æŸ¥è¯¢")
                continue
            
            print_stream(user_input)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºå·²ä¸­æ–­ï¼Œæ„Ÿè°¢ä½¿ç”¨ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜")

if __name__ == "__main__":
    main()
